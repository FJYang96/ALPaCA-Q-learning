{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Exploration through ALPaCA Q Learning\n",
    "Course project for CS332: Advanced Survey on Reinforcement Learning\n",
    "\n",
    "Fengjun Yang, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ALPaCA\n",
    "from agent.alpaca import ALPaCA\n",
    "from agent.alpacaQ import ALPaCAQ\n",
    "from agent.alpacaoffline import ALPaCAOffline\n",
    "\n",
    "# Mountain Car related\n",
    "from metamountaincar.mmcenv import MetaMountainCar as MMC\n",
    "from metamountaincar.utility import *\n",
    "from metamountaincar.VI import mmcVI\n",
    "from metamountaincar.mmcDataset import mmcDataset, MCOfflineDataset\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "cfg_filename = 'config.yml'\n",
    "with open(cfg_filename,'r') as ymlfile:\n",
    "    config = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate value functions for offline training. \n",
    "Technically, one can use mmcDataset directly for ALPaCA offline training. Here, however, we store and reuse sampled data.\n",
    "\n",
    "SKIP IF USING PRESAMPLED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATASET = 5                       # number of datasets to presample\n",
    "N_FUNCS = config['meta_batch_size'] # number of environments per dataset\n",
    "N_SAMPLES = 80                      # number of data points per environment\n",
    "DATA_DIR = config['data_dir']       # where to store the presampled datasets (has to be an existing directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Q functions and store in DATA_DIR\n",
    "for i in tqdm(range(N_DATASET)):\n",
    "    sample = mmcDataset(config).sample(N_FUNCS, N_SAMPLES)\n",
    "    with open(DATA_DIR+str(i)+'.yml', 'w') as yml_file:\n",
    "        yml_file.write(yaml.dump(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for offline training\n",
    "dataset = MCOfflineDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize offline alpaca agent\n",
    "offline = ALPaCAOffline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline training\n",
    "offline.train_offline(dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save agent in a specified directory\n",
    "offline.save_agent('./saveagent/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = MMC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 57)\n",
      "(0, 51)\n",
      "(0, 57)\n",
      "(0, 51)\n",
      "(0, 52)\n",
      "(0, 51)\n",
      "(0, 52)\n",
      "(0, 53)\n",
      "(0, 52)\n",
      "(0, 53)\n",
      "(0, 54)\n",
      "(0, 53)\n",
      "(0, 54)\n",
      "(0, 55)\n",
      "(0, 54)\n",
      "(0, 55)\n",
      "(0, 56)\n",
      "(0, 55)\n",
      "(0, 56)\n",
      "(0, 57)\n",
      "(0, 56)\n",
      "(0, 57)\n",
      "(0, 58)\n",
      "(0, 57)\n",
      "(0, 58)\n",
      "(0, 59)\n",
      "(0, 58)\n",
      "(0, 59)\n",
      "(0, 60)\n",
      "(0, 59)\n",
      "(0, 60)\n",
      "(0, 61)\n",
      "(0, 60)\n",
      "(0, 61)\n",
      "(0, 62)\n",
      "(0, 61)\n",
      "(0, 62)\n",
      "(0, 63)\n",
      "(0, 62)\n",
      "(0, 63)\n",
      "(1, 64)\n",
      "(0, 63)\n",
      "(1, 64)\n",
      "(1, 65)\n",
      "(1, 64)\n",
      "(1, 65)\n",
      "(1, 66)\n",
      "(1, 65)\n",
      "(1, 66)\n",
      "(1, 67)\n",
      "(1, 66)\n",
      "(1, 67)\n",
      "(1, 68)\n",
      "(1, 67)\n",
      "(1, 68)\n",
      "(1, 69)\n",
      "(1, 68)\n",
      "(1, 69)\n",
      "(1, 70)\n",
      "(1, 69)\n",
      "(1, 70)\n",
      "(1, 71)\n",
      "(1, 70)\n",
      "(1, 71)\n",
      "(1, 72)\n",
      "(1, 71)\n",
      "(1, 72)\n",
      "(1, 73)\n",
      "(1, 72)\n",
      "(1, 73)\n",
      "(1, 74)\n",
      "(1, 73)\n",
      "(1, 74)\n",
      "(1, 75)\n",
      "(1, 74)\n",
      "(1, 75)\n",
      "(1, 76)\n",
      "(1, 75)\n",
      "(1, 76)\n",
      "(2, 77)\n",
      "(1, 76)\n",
      "(2, 77)\n",
      "(2, 78)\n",
      "(2, 77)\n",
      "(2, 78)\n",
      "(2, 79)\n",
      "(2, 78)\n",
      "(2, 79)\n",
      "(2, 80)\n",
      "(2, 79)\n",
      "(2, 80)\n",
      "(2, 81)\n",
      "(2, 80)\n",
      "(2, 81)\n",
      "(2, 82)\n",
      "(2, 81)\n",
      "(2, 82)\n",
      "(2, 83)\n",
      "(2, 82)\n",
      "(2, 83)\n",
      "(2, 84)\n",
      "(2, 83)\n",
      "(2, 84)\n",
      "(2, 85)\n",
      "(2, 84)\n",
      "(2, 85)\n",
      "(2, 86)\n",
      "(2, 85)\n",
      "(2, 86)\n",
      "(2, 87)\n",
      "(2, 86)\n",
      "(2, 87)\n",
      "(2, 88)\n",
      "(2, 87)\n",
      "(2, 88)\n",
      "(2, 89)\n",
      "(2, 88)\n",
      "(2, 89)\n",
      "(3, 90)\n",
      "(2, 89)\n",
      "(3, 90)\n",
      "(3, 91)\n",
      "(3, 90)\n",
      "(3, 91)\n",
      "(3, 92)\n",
      "(3, 91)\n",
      "(3, 92)\n",
      "(3, 93)\n",
      "(3, 92)\n",
      "(3, 93)\n",
      "(3, 94)\n",
      "(3, 93)\n",
      "(3, 94)\n",
      "(3, 95)\n",
      "(3, 94)\n",
      "(3, 95)\n",
      "(3, 96)\n",
      "(3, 95)\n",
      "(3, 96)\n",
      "(3, 97)\n",
      "(3, 96)\n",
      "(3, 97)\n",
      "(3, 98)\n",
      "(3, 97)\n",
      "(3, 98)\n",
      "(3, 99)\n",
      "(3, 98)\n",
      "(3, 99)\n",
      "(3, 100)\n",
      "(3, 99)\n",
      "(3, 100)\n",
      "(3, 101)\n",
      "(3, 100)\n",
      "(3, 101)\n",
      "(3, 102)\n",
      "(3, 101)\n",
      "(3, 102)\n",
      "(4, 103)\n",
      "(4, 102)\n",
      "(4, 103)\n",
      "(4, 104)\n",
      "(4, 103)\n",
      "(4, 104)\n",
      "(4, 105)\n",
      "(4, 104)\n",
      "(4, 105)\n",
      "(4, 106)\n",
      "(4, 105)\n",
      "(4, 106)\n",
      "(4, 107)\n",
      "(4, 106)\n",
      "(4, 107)\n",
      "(4, 108)\n",
      "(4, 107)\n",
      "(4, 108)\n",
      "(4, 109)\n",
      "(4, 108)\n",
      "(4, 109)\n",
      "(4, 110)\n",
      "(4, 109)\n",
      "(4, 110)\n",
      "(4, 111)\n",
      "(4, 110)\n",
      "(4, 111)\n",
      "(4, 112)\n",
      "(4, 111)\n",
      "(4, 112)\n",
      "(4, 113)\n",
      "(4, 112)\n",
      "(4, 113)\n",
      "(4, 114)\n",
      "(4, 113)\n",
      "(4, 114)\n",
      "(4, 115)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 115 is out of bounds for axis 1 with size 115",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-112aed4c5ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_true_cost_to_go\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m115\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Research/ALPaCAQ/metamountaincar/utility.py\u001b[0m in \u001b[0;36mplot_true_cost_to_go\u001b[0;34m(env, gran, it, savedir)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m    110\u001b[0m     \u001b[0mVIagent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmcVI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgran\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgravity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgravity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthrust\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrust\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mVIagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mQ_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVIagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mV_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Research/ALPaCAQ/metamountaincar/VI.py\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mnew_q_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_limit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_q_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 115 is out of bounds for axis 1 with size 115"
     ]
    }
   ],
   "source": [
    "plot_true_cost_to_go(env, 115, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online prediction with analytical solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from agent\n",
    "K0 = sess1.run(agent.K)\n",
    "L0 = sess1.run(agent.L)\n",
    "L0_inv = np.linalg.inv(L0)\n",
    "Sig = sess1.run(agent.SigEps)\n",
    "encode = agent.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict a specific point\n",
    "state = np.array([0.5, 0.06])\n",
    "norm_ob = dataset.normalize_x(state[None,None,:])\n",
    "phi = encode(norm_ob)[0].T\n",
    "pred_mean = dataset.denormalize_y(K0.T @ phi)\n",
    "pred_var = (1 + phi.T @ L0_inv @ phi) * np.diag(Sig[0,0]) * (dataset.q_var ** 2)\n",
    "pred_mean.flatten(), pred_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the model with some test online data\n",
    "# Suppose online data is: (for action 0)\n",
    "# - (0.5,0.06) --> 1\n",
    "action = 0\n",
    "X = np.array([[0.5,0.06]]).T\n",
    "Y = np.array([[0]])\n",
    "# Preprocess the data\n",
    "norm_X = dataset.normalize_x(X.T[None,:,:])\n",
    "Phi = agent.encode(norm_X)[0].astype(np.float64)\n",
    "norm_Y = dataset.normalize_y(Y[None,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the same point after the observations using the analytical solution\n",
    "L_ana = (Phi.T @ Phi + L0).astype(np.float64)\n",
    "L_ana_inv = np.linalg.inv(L_ana)\n",
    "K_ana = K0.copy().astype(np.float64)\n",
    "Q_ana = Phi.T @ norm_Y + (L0 @ (K0[:, action]))[:, None]\n",
    "K_ana[:, action] = (L_ana_inv @ Q_ana)[0,:,0]\n",
    "pred_mean = dataset.denormalize_y(K_ana.T @ phi)\n",
    "pred_var = (1 + phi.T @ L_ana_inv @ phi) * np.diag(Sig[0,0]) * (dataset.q_var ** 2)\n",
    "pred_mean.flatten(), pred_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online prediction with incremental update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the same point after incremental update\n",
    "aq = ALPaCA_Q(agent, None, config, sess1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.update_model(X.T[0],action,norm_Y[0])\n",
    "target = dataset.normalize_y(0)\n",
    "aq.predict_q_values(state).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq.predict_var(state) * dataset.q_var ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot true q-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample or specify a gravity - thrust pair\n",
    "gravity, thrust = 0.0025, 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VI for this configuration\n",
    "gran = 150\n",
    "VIagent = MCVI(gran, 200, gravity=grav, thrust=thrust)\n",
    "VIagent.value_iteration()\n",
    "Q_table = VIagent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_true_cost_to_go(Q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot learned q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learned_cost_to_go(ALPaCA_Q, grid, savedir=None):\n",
    "    \"\"\"\n",
    "    Plot the learned cost to go function for an ALPaCA_Q agent\n",
    "    \"\"\"\n",
    "    # Process the data for plotting\n",
    "    X = np.arange(-1.2, 0.6, 1.8/grid)\n",
    "    Y = np.arange(-0.07, 0.07, 0.14/grid)\n",
    "    X,Y = np.meshgrid(X,Y)\n",
    "    # Fill in the table by iteratively querying the agent\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            Z[i,j] = ALPaCA_Q.predict_q_values(np.array([X[i,j],Y[i,j]])).max()\n",
    "    # Plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X,Y,-Z, cmap=cm.coolwarm)\n",
    "    if savedir is not None:\n",
    "        fig.savefig(savedir,transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learned_variance(ALPaCA_Q, grid, savedir=None):\n",
    "    \"\"\"\n",
    "    Plot the learned cost to go function for an ALPaCA_Q agent\n",
    "    \"\"\"\n",
    "    # Process the data for plotting\n",
    "    X = np.arange(-1.2, 0.6, 1.8/grid)\n",
    "    Y = np.arange(-0.07, 0.07, 0.14/grid)\n",
    "    X,Y = np.meshgrid(X,Y)\n",
    "    # Fill in the table by iteratively querying the agent\n",
    "    Z = np.zeros(X.shape)\n",
    "    for i in range(grid):\n",
    "        for j in range(grid):\n",
    "            s = np.array([X[i,j],Y[i,j]])\n",
    "            Z[i,j] = ALPaCA_Q.predict_var(s).mean()\n",
    "    # Plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X,Y,np.sqrt(Z), cmap=cm.coolwarm)\n",
    "    if savedir is not None:\n",
    "        fig.savefig(savedir,transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an agent\n",
    "agent.restore('./Pretrained_alpaca/norm_y_2/')\n",
    "aq = ALPaCA_Q(agent, None, config, sess1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_learned_cost_to_go(MCaq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learned_variance(aq,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Experiment / RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(result):\n",
    "    avg_result = result.mean(0)\n",
    "    ind = np.arange(len(avg_result)) + 1\n",
    "    plt.plot(ind, avg_result)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment and ALPaCA Q agent\n",
    "env = MCF()\n",
    "agent.restore('./Pretrained_alpaca/norm_y_2/')\n",
    "aq = ALPaCA_Q(agent, None, config, sess1)\n",
    "MCaq = ALPaCA_Q(agent, env, config, sess1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grav, thru = (0.0021448523969069813, 0.0012198799893080752)\n",
    "env1 = MCF(gravity=grav, thrust=thru)\n",
    "grav, thru = (0.0023464856819864063, 0.0013694856651231633)\n",
    "env2 = MCF(gravity=grav, thrust=thru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "step_limit = 500\n",
    "for i in range(num_episodes):\n",
    "    MCaq.ts_non_bootstrap(step_limit)\n",
    "    print(play_episode(env, MCaq, render=False,it=step_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_learned_cost_to_go(MCaq, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# E-greedy learning env1\n",
    "num_experiments = 20\n",
    "num_episodes = 10\n",
    "num_plays = 3\n",
    "step_limit = 200\n",
    "env = env1\n",
    "e_greedy_results_env1 = np.zeros((num_experiments, num_episodes))\n",
    "# Run experiment\n",
    "for i in tqdm(range(num_experiments)):\n",
    "    MCaq = ALPaCA_Q(agent, env, config, sess1) #Reinitialize agent for each experiment\n",
    "    for j in range(num_episodes):\n",
    "        train_step = MCaq.e_greedy_train_episode(step_limit,render=False)\n",
    "        # Play several games to find average steps\n",
    "        play_step = []\n",
    "        for k in range(num_plays):\n",
    "            play_step.append(play_episode(env, MCaq, render=False,it=step_limit))\n",
    "        e_greedy_results_env1[i][j] = np.mean(play_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# E-greedy learning env2\n",
    "num_experiments = 20\n",
    "num_episodes = 1000\n",
    "num_plays = 3\n",
    "step_limit = 200\n",
    "env = env2\n",
    "e_greedy_results_env2 = np.zeros((num_experiments, num_episodes))\n",
    "# Run experiment\n",
    "for i in tqdm(range(num_experiments)):\n",
    "    MCaq = ALPaCA_Q(agent, env, config, sess1) #Reinitialize agent for each experiment\n",
    "    for j in range(num_episodes):\n",
    "        train_step = MCaq.e_greedy_train_episode(step_limit,render=False)\n",
    "        # Play several games to find average steps\n",
    "        play_step = []\n",
    "        for k in range(num_plays):\n",
    "            play_step.append(play_episode(env, MCaq, render=False,it=step_limit))\n",
    "        e_greedy_results_env2[i][j] = np.mean(play_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TS learning env1\n",
    "num_experiments = 10\n",
    "num_episodes = 1000\n",
    "num_plays = 3\n",
    "step_limit = 200\n",
    "env = env1\n",
    "# Array to store results\n",
    "TS_results_env1 = np.zeros((num_experiments, num_episodes))\n",
    "# Run experiment\n",
    "for i in tqdm(range(num_experiments)):\n",
    "    MCaq = ALPaCA_Q(agent, env, config, sess1)\n",
    "    for j in range(num_episodes):\n",
    "        train_step = MCaq.ts_train_episode(step_limit,render=False)# Train an episode and get data\n",
    "        # Play several games to find average steps\n",
    "        play_step = []\n",
    "        for k in range(num_plays):\n",
    "            play_step.append(play_episode(env, MCaq, render=False,it=step_limit))\n",
    "        TS_results_env1[i][j] = np.mean(play_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_summary(TS_results_env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TS learning env2\n",
    "num_experiments = 10\n",
    "num_episodes = 1000\n",
    "num_plays = 3\n",
    "step_limit = 200\n",
    "env = env2\n",
    "# Array to store results\n",
    "TS_results_env2 = np.zeros((num_experiments, num_episodes))\n",
    "# Run experiment\n",
    "for i in tqdm(range(num_experiments)):\n",
    "    MCaq = ALPaCA_Q(agent, env, config, sess1)\n",
    "    for j in range(num_episodes):\n",
    "        train_step = MCaq.ts_train_episode(step_limit,render=False)# Train an episode and get data\n",
    "        # Play several games to find average steps\n",
    "        play_step = []\n",
    "        for k in range(num_plays):\n",
    "            play_step.append(play_episode(env, MCaq, render=False,it=step_limit))\n",
    "        TS_results_env2[i][j] = np.mean(play_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_1 = np.loadtxt('Plots/egreedyresult1backup.txt').mean(0)\n",
    "ts_1 = TS_results_env1.mean(0)\n",
    "ind = np.arange(1000) + 1\n",
    "l1, = plt.plot(ind, eg_1)\n",
    "l2, = plt.plot(ind, ts_1)\n",
    "l3, = plt.plot(ind, [90]*1000, '-')\n",
    "plt.legend([l1, l2, l3], ['$\\epsilon$-greedy', 'Thompson sampling', 'Optimal policy'])\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average steps per episode')\n",
    "plt.savefig('Plots/RL_result1.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCaq.L_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Plots/TS-results-backup.txt',TS_results_env1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summary(e_greedy_results_backup_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learned_cost_to_go(MCaq, grid=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learned_variance(MCaq,grid=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e_greedy_summary = e_greedy_results_env2[:18].mean(0)\n",
    "plt.plot(np.arange(num_episodes)+1, e_greedy_summary)\n",
    "#plt.savefig('Plots/e-greedy-steps0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env1_params_backup\n",
    "#env2_params_backup\n",
    "#e_greedy_results_backup_1 #500 episode original env\n",
    "#e_greedy_results_backup_2 #1000 episode env1, 20 experiments, epsilon=0.3\n",
    "#e_greedy_results_backup_3 #1000 episode env2, 19 experiment, epsilon=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "env1.close()\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
